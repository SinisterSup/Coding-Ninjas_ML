{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Decision Tree Implementation</h1><br>\n",
    "<p>\n",
    "    <ul>\n",
    "        <li>I have created the actual decision tree itself. I did not do \"creating the decision tree\" and \"printing as in the given example\" separately. I created the decision tree and fitted it with the iris dataset and printed the score too.</li>\n",
    "        <li>The score is based on the mean accuracy</li>\n",
    "    </ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(iris.data)\n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find label for a value\n",
    "#if MIN_Value <=val < (m + Mean_Value) / 2 then it is assigned label a\n",
    "#if (m + Mean_Value) <=val < Mean_Value then it is assigned label b\n",
    "#if (Mean_Value) <=val < (Mean_Value + MAX_Value)/2 then it is assigned label c\n",
    "#if (Mean_Value + MAX_Value)/2 <=val <= MAX_Value  then it is assigned label d\n",
    "\n",
    "def label(val, *boundaries):\n",
    "    if (val < boundaries[0]):\n",
    "        return 'a'\n",
    "    elif (val < boundaries[1]):\n",
    "        return 'b'\n",
    "    elif (val < boundaries[2]):\n",
    "        return 'c'\n",
    "    else:\n",
    "        return 'd'\n",
    "\n",
    "#Function to convert a continuous data into labelled data\n",
    "#There are 4 lables  - a, b, c, d\n",
    "def toLabel(df, old_feature_name):\n",
    "    second = df[old_feature_name].mean()\n",
    "    minimum = df[old_feature_name].min()\n",
    "    first = (minimum + second)/2\n",
    "    maximum = df[old_feature_name].max()\n",
    "    third = (maximum + second)/2\n",
    "    return df[old_feature_name].apply(label, args= (first, second, third))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sl</th>\n",
       "      <th>sw</th>\n",
       "      <th>pl</th>\n",
       "      <th>pw</th>\n",
       "      <th>sl_labeled</th>\n",
       "      <th>sw_labeled</th>\n",
       "      <th>pl_labeled</th>\n",
       "      <th>pw_labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>c</td>\n",
       "      <td>a</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>c</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sl   sw   pl   pw sl_labeled sw_labeled pl_labeled pw_labeled\n",
       "0    5.1  3.5  1.4  0.2          b          c          a          a\n",
       "1    4.9  3.0  1.4  0.2          a          b          a          a\n",
       "2    4.7  3.2  1.3  0.2          a          c          a          a\n",
       "3    4.6  3.1  1.5  0.2          a          c          a          a\n",
       "4    5.0  3.6  1.4  0.2          a          c          a          a\n",
       "..   ...  ...  ...  ...        ...        ...        ...        ...\n",
       "145  6.7  3.0  5.2  2.3          c          b          c          d\n",
       "146  6.3  2.5  5.0  1.9          c          a          c          d\n",
       "147  6.5  3.0  5.2  2.0          c          b          c          d\n",
       "148  6.2  3.4  5.4  2.3          c          c          d          d\n",
       "149  5.9  3.0  5.1  1.8          c          b          c          c\n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert all columns to labelled data\n",
    "df['sl_labeled'] = toLabel(df, 'sl')\n",
    "df['sw_labeled'] = toLabel(df, 'sw')\n",
    "df['pl_labeled'] = toLabel(df, 'pl')\n",
    "df['pw_labeled'] = toLabel(df, 'pw')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['sl', 'sw', 'pl', 'pw'], axis = 1, inplace = True)\n",
    "set(df['sl_labeled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, data,output):\n",
    "        # data is the feature upon which the node is going to split when fitting the training data and it is none for leaf node.\n",
    "        self.data = data\n",
    "        # i am storing the children of a node as a dictionary where key = value of feature upon which the node has splitted\n",
    "        # and the corresponding value stores the child TreeNode\n",
    "        self.children = {} #dictionary as mentioned above\n",
    "        # output represents the class with current majority at this instance of the decision tree\n",
    "        self.output = output\n",
    "        # \"index\" variable assigns a perticular index to each node\n",
    "        self.index = -1\n",
    "        \n",
    "    def add_child(self,feature_value,object):\n",
    "        self.children[feature_value] = object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_tree:\n",
    "    def __init__(self):\n",
    "        # root node of our decision tree\n",
    "        self.__root = None\n",
    "\n",
    "    def __frequency_counter(self,Y):\n",
    "        # returns a dictionary with keys as unique values of Y(i.e no of classes) and the corresponding value as its frequency\n",
    "        d = {}\n",
    "        for i in Y:\n",
    "            if i in d:\n",
    "                d[i]+=1\n",
    "            else:\n",
    "                d[i]=1\n",
    "        return d\n",
    "\n",
    "\n",
    "    def __entropy(self,Y):\n",
    "        # returns the entropy of that perticular node.\n",
    "        freq_map = self.__frequency_counter(Y)\n",
    "        entropy_ = 0\n",
    "        total = len(Y)\n",
    "        for i in freq_map:\n",
    "            p = freq_map[i]/total\n",
    "            entropy_ += (-p)*math.log2(p)\n",
    "            #note that i have used log2 and not log10.\n",
    "        return entropy_\n",
    "\n",
    "    def __gain_ratio(self,X,Y,selected_feature):\n",
    "        # returns the gain ratio\n",
    "        original = self.__entropy(Y) # \"original\" represents entropy before splitting\n",
    "        entropy_after_splitting = 0 \n",
    "        split_info = 0\n",
    "        values = set(X[:,selected_feature])\n",
    "        df = pd.DataFrame(X)\n",
    "        # Appending Y values as the last column in the newely created dataframe \n",
    "        df[df.shape[1]] = Y\n",
    "        starting_size = df.shape[0] \n",
    "        for i in values:\n",
    "            df1 = df[df[selected_feature] == i]\n",
    "            current_size = df1.shape[0]\n",
    "            entropy_after_splitting += (current_size/starting_size)*self.__entropy(df1[df1.shape[1]-1])\n",
    "            split_info += (-current_size/starting_size)*math.log2(current_size/starting_size)\n",
    "\n",
    "        # when split info is zero, it will give an error. to handle that we have following piece of code\n",
    "        if split_info == 0 :\n",
    "            return math.inf \n",
    "        #returning negative infinity when split info is 0\n",
    "\n",
    "        info_gain = original - entropy_after_splitting\n",
    "        gain_ratio = info_gain / split_info\n",
    "        return gain_ratio #returned the gain_ratio finally\n",
    "\n",
    "\n",
    "\n",
    "    def __decision_tree(self,X,Y,features,level,classes, all_features=np.array([i for i in df.columns])):\n",
    "        # returns the root of the Decision Tree built after fitting the training data\n",
    "        # classes = all the different classes available to us\n",
    "        # level = depth of the tree\n",
    "        # traversal will be in preorder fashion\n",
    "        \n",
    "#                 If we run out of features to split upon, then the answer will be the class whose count will be the maximum for the given split\n",
    "#         If the length of unused features is zero, you will have to:\n",
    "#         1. print the level\n",
    "#         2. count the frequency of each class in Y, and store it in a dictionary, lets name it freqs\n",
    "#         3. now iterate in each of these unique classes and if you don't find that class in freqs then print \"count of (class_name) =0\" else if you find that particular class in the freqs dictionary, for this case just initialize two variables outside the for loop claaed output and maxcount. maxcount will store the count of the frequency of the class that has the maximum frequency in the freqs dict.\n",
    "\n",
    "#         it should look something like this\n",
    "#         for i in classes:\n",
    "#             if i not in freqs:\n",
    "#                 print(\"Count of\",i,\"=\",0)\n",
    "#             else :\n",
    "#                 if freqs[i] > max_count :\n",
    "#                     output = i\n",
    "#                     max_count = freqs[i]\n",
    "#                 print(\"Count of\",i,\"=\",freqs[i])\n",
    "\n",
    "\n",
    "#         when you are finished printing the class with maximum freq, then you have to print the current entropy\n",
    "#         and then print \"Reached Leaf Node\"\n",
    "#         and then return. this will be the base case, when you have run out of features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         now lets talk about the second base case, i.e. when the node consists of a single pure class.\n",
    "#         if len(set(Y))==1\n",
    "#         in that case, you will have to print the current level.\n",
    "#         1. iterate in all the classes and whenever you find that unique class (the class in y) in the all possible classes, then you will have to print the count of that class(which will be len(y))\n",
    "#         or if you dont find it in the classes then just print count of (that class which you didnt find) =0.\n",
    "#         also, print \"current entropy=0.0\"\n",
    "#         print \"Reached leaf node\"\n",
    "#         and then return\n",
    "\n",
    "#         the iteration part will look somethinglike this\n",
    "#         for class_ in classes:\n",
    "#             if class_ in Y:\n",
    "#                 output = class_\n",
    "#                 print(\"Count of\",class_,\"=\",len(Y))\n",
    "#             else :\n",
    "#                 print(\"Count of\",class_,\"=\",0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         this is all about the base cases\n",
    "        \n",
    "        #BASE CASES\n",
    "        # first stopping criteria: If we have run out of features to split upon\n",
    "        # In this case the answer will be the class whose count is maximum for the given split\n",
    "        if len(features) == 0:\n",
    "            print(\"Level\",level)\n",
    "            freqs = self.__frequency_counter(Y)\n",
    "            output = None\n",
    "            max_count = -99999999999999\n",
    "            for i in classes:\n",
    "                if i not in freqs:\n",
    "                    print(\"Count of\",i,\"=\",0)\n",
    "                else :\n",
    "                    if freqs[i] > max_count :\n",
    "                        output = i\n",
    "                        max_count = freqs[i]\n",
    "                    print(\"Count of\",i,\"=\",freqs[i])\n",
    "\n",
    "            print(\"Current Entropy  is =\",self.__entropy(Y))          \n",
    "\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "        \n",
    "        \n",
    "        # second stopping criteria: If the node consists of a single pure class\n",
    "        if len(set(Y)) == 1:\n",
    "            print(\"Level\",level)\n",
    "            output = None\n",
    "            for class_ in classes:\n",
    "                if class_ in Y:\n",
    "                    output = class_\n",
    "                    print(\"Count of\",class_,\"=\",len(Y))\n",
    "                else :\n",
    "                    print(\"Count of\",class_,\"=\",0)\n",
    "            print(\"Current Entropy is =  0.0\")\n",
    "            print(\"Reached leaf Node\")\n",
    "            print()\n",
    "            return TreeNode(None,output)\n",
    "\n",
    "        #now starts the backbone of our decision tree classifier.\n",
    "        # after the base cases we have to Find the best feature to split upon\n",
    "        max_gain = -math.inf\n",
    "        final_feature = None\n",
    "        for f in features :\n",
    "            current_gain = self.__gain_ratio(X,Y,f)\n",
    "\n",
    "            if current_gain > max_gain:\n",
    "                max_gain = current_gain\n",
    "                final_feature = f\n",
    "\n",
    "        print(\"Level\",level)\n",
    "        freqs = self.__frequency_counter(Y)\n",
    "        output = None\n",
    "        max_count = -math.inf\n",
    "\n",
    "        for class_ in classes:\n",
    "            if class_ not in freqs:\n",
    "                print(\"Count of\",class_,\"=\",0)\n",
    "            else :\n",
    "                if freqs[class_] > max_count :\n",
    "                    output = class_\n",
    "                    max_count = freqs[class_]\n",
    "                print(\"Count of\",class_,\"=\",freqs[class_])\n",
    "        \n",
    "        print(\"Current Entropy is =\",self.__entropy(Y))\n",
    "        print(\"Splitting on feature\" , all_features[final_feature] , \"with gain ratio\" , max_gain)\n",
    "        print()\n",
    "        \n",
    "\n",
    "            \n",
    "        unique_values = set(X[:,final_feature]) # unique_values represents the unique values of the feature selected\n",
    "        df = pd.DataFrame(X)\n",
    "        # appending y values at the end of the data frame.\n",
    "        df[df.shape[1]] = Y\n",
    "\n",
    "        current_node = TreeNode(final_feature,output)\n",
    "\n",
    "        # removing the selected feature from the list.\n",
    "        index  = features.index(final_feature)\n",
    "        features.remove(final_feature)\n",
    "        for i in unique_values:\n",
    "            # Creating a new dataframe with value of selected feature = i\n",
    "            df_new = df[df[final_feature] == i]\n",
    "            # recursively calling on the splits\n",
    "            node = self.__decision_tree(df_new.iloc[:,0:df_new.shape[1]-1].values,df_new.iloc[:,df_new.shape[1]-1].values,features,level+1,classes)\n",
    "            current_node.add_child(i,node)\n",
    "\n",
    "        # Add the removed feature     \n",
    "        features.insert(index,final_feature)\n",
    "\n",
    "        return current_node\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        # Fits according to the given training data\n",
    "        features = [i for i in range(len(X[0]))]\n",
    "        classes = set(Y)\n",
    "        level = 0\n",
    "        self.__root = self.__decision_tree(X,Y,features,level,classes)\n",
    "        \n",
    "    def __predict_for_single_point(self,data,node):\n",
    "        # predicts the class for a given testing point\n",
    "        # We have reached a leaf node\n",
    "        if len(node.children) == 0 :\n",
    "            return node.output\n",
    "\n",
    "        val = data[node.data] # represents the value of feature on which the split is made       \n",
    "        if val not in node.children :\n",
    "            return node.output\n",
    "        \n",
    "        # Recursively call on the splits\n",
    "        return self.__predict_for_single_point(data,node.children[val])\n",
    "\n",
    "    def predict(self,X):\n",
    "        # This function returns Y-predicted\n",
    "        # X is a 2D numpy array\n",
    "        Y = np.array([0 for i in range(len(X))])\n",
    "        for i in range(len(X)):\n",
    "            Y[i] = self.__predict_for_single_point(X[i],self.__root)\n",
    "        return Y\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        # this function returns the mean accuracy.\n",
    "        Y_predicted = self.predict(X)\n",
    "        counter = 0\n",
    "        for i in range(len(Y_predicted)):\n",
    "            if Y_predicted[i] == Y[i]:\n",
    "                counter+=1\n",
    "        return counter/len(Y_predicted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x = df.values\n",
    "y = iris.target\n",
    "x_train, x_test, y_train, y_test=train_test_split(x, y,random_state = 1)\n",
    "unused_features = set(df.columns)\n",
    "clf = build_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score= 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n",
    "print()\n",
    "print(\"Score=\", clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy seems pretty good. you can test it on various datasets by converting the x and y values to the numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This build_tree instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_export.py:180\u001b[0m, in \u001b[0;36mplot_tree\u001b[1;34m(decision_tree, max_depth, feature_names, class_names, label, filled, impurity, node_ids, proportion, rounded, precision, ax, fontsize)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_tree\u001b[39m(\n\u001b[0;32m     79\u001b[0m     decision_tree,\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m     fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     93\u001b[0m ):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124;03m\"\"\"Plot a decision tree.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    The sample counts that are shown are weighted with any sample_weights that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     exporter \u001b[38;5;241m=\u001b[39m _MPLTreeExporter(\n\u001b[0;32m    183\u001b[0m         max_depth\u001b[38;5;241m=\u001b[39mmax_depth,\n\u001b[0;32m    184\u001b[0m         feature_names\u001b[38;5;241m=\u001b[39mfeature_names,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    193\u001b[0m         fontsize\u001b[38;5;241m=\u001b[39mfontsize,\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exporter\u001b[38;5;241m.\u001b[39mexport(decision_tree, ax\u001b[38;5;241m=\u001b[39max)\n",
      "File \u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1222\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1217\u001b[0m     fitted \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1218\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1219\u001b[0m     ]\n\u001b[0;32m   1221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[1;32m-> 1222\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This build_tree instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz #to visualize decision tree\n",
    "import pydotplus\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(clf, out_file=None,\n",
    "                           feature_names=iris.feature_names,\n",
    "                           class_names=iris.target_names, \n",
    "                           filled=True, rounded=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pydotplus.graph_from_dot_data(dot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(graph.create_png())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
